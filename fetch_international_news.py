#!/usr/bin/env python
"""
å›½é™…æ–°é—»æºæ•°æ®è·å–å™¨
æ”¯æŒReuters RSSã€SEC.govç­‰å›½é™…æƒå¨æº
"""

import json
import time
import requests
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
import re

try:
    import feedparser
    FEEDPARSER_AVAILABLE = True
except ImportError:
    FEEDPARSER_AVAILABLE = False
    print("Warning: feedparser not available, RSS support limited")

try:
    from bs4 import BeautifulSoup
    BS4_AVAILABLE = True
except ImportError:
    BS4_AVAILABLE = False
    print("Warning: BeautifulSoup not available, web scraping disabled")

@dataclass
class NewsItem:
    """æ–°é—»æ¡ç›®æ•°æ®ç»“æ„"""
    title: str
    url: str
    content: str = ""
    summary: str = ""  # æ‘˜è¦
    full_content: str = ""  # å®Œæ•´å†…å®¹
    published: Optional[datetime] = None
    source: str = ""
    categories: List[str] = None
    author: str = ""
    tags: List[str] = None
    
    def __post_init__(self):
        if self.categories is None:
            self.categories = []
        if self.tags is None:
            self.tags = []

class InternationalNewsAggregator:
    """å›½é™…æ–°é—»èšåˆå™¨"""
    
    def __init__(self, config_path: str = "international_sources.json"):
        """åˆå§‹åŒ–èšåˆå™¨"""
        self.config_path = Path(config_path)
        self.config = self._load_config()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': self.config['settings']['user_agent']
        })
        
    def _load_config(self) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"Config file {self.config_path} not found")
            return {"international_sources": {}, "settings": {}}
    
    def fetch_rss_feed(self, feed_url: str, timeout: int = 30) -> List[NewsItem]:
        """è·å–RSSæºæ•°æ®"""
        if not FEEDPARSER_AVAILABLE:
            print(f"RSS support not available, skipping {feed_url}")
            return []
            
        try:
            print(f"Fetching RSS: {feed_url}")
            
            # è®¾ç½®feedparserçš„ç”¨æˆ·ä»£ç†
            feedparser.USER_AGENT = self.config['settings']['user_agent']
            
            # è·å–RSSå†…å®¹
            feed = feedparser.parse(feed_url)
            
            if feed.bozo:
                print(f"RSS feed may have issues: {feed_url}")
            
            items = []
            max_items = self.config['settings']['max_articles_per_source'] * 2  # å¢åŠ æ•°é‡
            
            for entry in feed.entries[:max_items]:
                # è§£æå‘å¸ƒæ—¶é—´
                published = None
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    published = datetime(*entry.published_parsed[:6])
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    published = datetime(*entry.updated_parsed[:6])
                
                # è·å–å†…å®¹ - å¢å¼ºç‰ˆæœ¬
                content = ""
                summary = ""
                
                if hasattr(entry, 'content') and entry.content:
                    # ä¼˜å…ˆä½¿ç”¨å®Œæ•´å†…å®¹
                    if isinstance(entry.content, list):
                        content = entry.content[0].value if entry.content else ""
                    else:
                        content = str(entry.content)
                
                if hasattr(entry, 'summary'):
                    summary = entry.summary
                elif hasattr(entry, 'description'):
                    summary = entry.description
                
                # å¦‚æœcontentä¸ºç©ºï¼Œä½¿ç”¨summary
                if not content:
                    content = summary
                
                # æ¸…ç†HTMLæ ‡ç­¾
                if content:
                    content = self._clean_html(content)
                if summary:
                    summary = self._clean_html(summary)
                
                # å°è¯•è·å–å®Œæ•´æ–‡ç« å†…å®¹
                full_content = ""
                if hasattr(entry, 'link') and entry.link:
                    full_content = self._fetch_article_content(entry.link, timeout)
                
                # è·å–ä½œè€…ä¿¡æ¯
                author = ""
                if hasattr(entry, 'author'):
                    author = entry.author
                elif hasattr(entry, 'authors') and entry.authors:
                    author = entry.authors[0] if entry.authors else ""
                
                # è·å–æ ‡ç­¾
                tags = []
                if hasattr(entry, 'tags') and entry.tags:
                    tags = [tag.term for tag in entry.tags if hasattr(tag, 'term')]
                
                item = NewsItem(
                    title=entry.title if hasattr(entry, 'title') else "",
                    url=entry.link if hasattr(entry, 'link') else "",
                    content=content,
                    summary=summary,
                    full_content=full_content,
                    published=published,
                    source=feed.feed.title if hasattr(feed.feed, 'title') else "",
                    categories=[],
                    author=author,
                    tags=tags
                )
                
                items.append(item)
                print(f"  Found: {item.title[:50]}... (Content: {len(item.full_content or item.content)} chars)")
            
            print(f"Retrieved {len(items)} items from {feed_url}")
            return items
            
        except Exception as e:
            print(f"Error fetching RSS {feed_url}: {e}")
            return []
    
    def fetch_web_content(self, url: str, timeout: int = 30) -> List[NewsItem]:
        """è·å–ç½‘é¡µå†…å®¹ (ç®€å•å®ç°)"""
        if not BS4_AVAILABLE:
            print(f"Web scraping not available, skipping {url}")
            return []
            
        try:
            print(f"Fetching web content: {url}")
            
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç®€å•çš„æ–°é—»æå–é€»è¾‘ (å¯æ ¹æ®å…·ä½“ç½‘ç«™ä¼˜åŒ–)
            items = []
            
            # æŸ¥æ‰¾æ–°é—»é“¾æ¥ (é€šç”¨æ–¹æ³•)
            links = soup.find_all('a', href=True)
            news_links = []
            
            for link in links[:50]:  # é™åˆ¶å¤„ç†çš„é“¾æ¥æ•°é‡
                href = link.get('href')
                text = link.get_text(strip=True)
                
                if (text and len(text) > 20 and 
                    any(keyword in text.lower() for keyword in 
                        ['market', 'stock', 'finance', 'economy', 'business', 'technology'])):
                    
                    if href.startswith('/'):
                        href = requests.urljoin(url, href)
                    
                    news_links.append((text, href))
            
            # å¤„ç†æ‰¾åˆ°çš„æ–°é—»
            for title, news_url in news_links[:10]:  # é™åˆ¶è·å–æ•°é‡
                item = NewsItem(
                    title=title,
                    url=news_url,
                    content="",  # å¯ä»¥è¿›ä¸€æ­¥è·å–å†…å®¹
                    published=datetime.now(),
                    source=url
                )
                items.append(item)
                print(f"  Found: {title[:50]}...")
            
            print(f"Retrieved {len(items)} items from {url}")
            return items
            
        except Exception as e:
            print(f"Error fetching web content {url}: {e}")
            return []
    
    def _fetch_article_content(self, url: str, timeout: int = 15) -> str:
        """æŠ“å–æ–‡ç« å®Œæ•´å†…å®¹"""
        if not BS4_AVAILABLE:
            return ""
        
        try:
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()
            
            # å°è¯•å¤šç§å†…å®¹æå–ç­–ç•¥
            content_selectors = [
                'article',
                '.article-content', 
                '.content',
                '.post-content',
                '.entry-content',
                '.story-body',
                '.article-body',
                '[data-module="ArticleBody"]',
                '.StandardArticleBody_body',
                'div[data-testid="paragraph"]'
            ]
            
            content_text = ""
            
            # å°è¯•æ¯ä¸ªé€‰æ‹©å™¨
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content_text = ' '.join([elem.get_text(strip=True) for elem in elements])
                    if len(content_text) > 200:  # å¦‚æœæ‰¾åˆ°è¶³å¤Ÿé•¿çš„å†…å®¹å°±åœæ­¢
                        break
            
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æ®µè½æ ‡ç­¾
            if len(content_text) < 200:
                paragraphs = soup.find_all('p')
                if paragraphs:
                    para_text = ' '.join([p.get_text(strip=True) for p in paragraphs[:10]])
                    if len(para_text) > len(content_text):
                        content_text = para_text
            
            # æ¸…ç†æ–‡æœ¬
            content_text = re.sub(r'\s+', ' ', content_text).strip()
            
            # é™åˆ¶é•¿åº¦
            if len(content_text) > 2000:
                content_text = content_text[:2000] + "..."
            
            return content_text
            
        except Exception as e:
            print(f"  Error fetching content from {url}: {e}")
            return ""
    
    def _clean_html(self, text: str) -> str:
        """æ¸…ç†HTMLæ ‡ç­¾"""
        if not text:
            return ""
        
        # ç®€å•çš„HTMLæ ‡ç­¾æ¸…ç†
        text = re.sub(r'<[^>]+>', '', text)
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'&[^;]+;', '', text)  # ç§»é™¤HTMLå®ä½“
        return text.strip()
    
    def fetch_all_sources(self) -> Dict[str, List[NewsItem]]:
        """è·å–æ‰€æœ‰å¯ç”¨æºçš„æ•°æ®"""
        all_items = {}
        
        sources = self.config.get('international_sources', {})
        
        for source_id, source_config in sources.items():
            if not source_config.get('enabled', True):
                print(f"Skipping disabled source: {source_id}")
                continue
            
            print(f"\\nProcessing source: {source_config['name']}")
            source_items = []
            
            source_type = source_config.get('type', 'rss')
            feeds = source_config.get('feeds', [])
            timeout = source_config.get('timeout', self.config['settings']['default_timeout'])
            
            for feed in feeds:
                feed_url = feed['url']
                
                if source_type == 'rss':
                    items = self.fetch_rss_feed(feed_url, timeout)
                elif source_type == 'web':
                    items = self.fetch_web_content(feed_url, timeout)
                elif source_type == 'api':
                    print(f"API support for {source_id} not yet implemented")
                    continue
                else:
                    print(f"Unknown source type: {source_type}")
                    continue
                
                # æ·»åŠ æºä¿¡æ¯å’Œåˆ†ç±»ä¿¡æ¯
                for item in items:
                    item.source = f"{source_config['name']} - {feed['name']}"
                    if 'sectors' in feed:
                        item.categories.extend(feed['sectors'])
                
                source_items.extend(items)
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                time.sleep(1)
            
            all_items[source_id] = source_items
            print(f"Source {source_id}: {len(source_items)} total items")
        
        return all_items
    
    def save_to_files(self, items_by_source: Dict[str, List[NewsItem]], 
                     output_dir: str = "output") -> List[str]:
        """ä¿å­˜åˆ°æ–‡ä»¶ï¼Œä½¿ç”¨å¸‚åœº+è¡Œä¸šåˆ†ç±»"""
        try:
            from topic_classifier import classify_market_and_sector
            CLASSIFICATION_AVAILABLE = True
        except ImportError:
            print("Warning: topic_classifier not available, using basic classification")
            CLASSIFICATION_AVAILABLE = False
        
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
            classified_items = {}  # ç”¨äºå­˜å‚¨åˆ†ç±»çš„æ¡ç›®
        saved_files = []
        
        # æŒ‰åˆ†ç±»ç»„ç»‡æ–‡ä»¶
        classified_items = {}
        
        for source_id, items in items_by_source.items():
            for item in items:
                if CLASSIFICATION_AVAILABLE:
                    # ä½¿ç”¨ç°æœ‰çš„åˆ†ç±»ç³»ç»Ÿ
                    market, sectors = classify_market_and_sector(item.url, item.title, item.content)
                    
                    # å›½é™…æºé»˜è®¤å½’ç±»ä¸ºINTLå¸‚åœºï¼Œä½†ä¹Ÿå°è¯•å…·ä½“å¸‚åœºè¯†åˆ«
                    if market == 'CN':
                        # å›½é™…æºä¸­çš„ä¸­å›½ç›¸å…³æ–°é—»
                        market = 'INTL_CN'
                    elif market in ['HK', 'US']:
                        # ä¿æŒåŸæœ‰åˆ†ç±»ï¼Œä½†æ ‡è®°ä¸ºå›½é™…æº
                        market = f'INTL_{market}'
                    else:
                        market = 'INTL'
                        
                    if not sectors:
                    # ä¸ºæ¯ä¸ªè¯†åˆ«çš„è¡Œä¸šåˆ›å»ºæ¡ç›®ï¼ˆä½¿ç”¨æ–‡ç« å‘å¸ƒæ—¥æœŸä½œä¸ºæ–‡ä»¶æ—¥æœŸï¼‰
                    for sector in sectors:
                        file_date = (item.published.date().isoformat() if item.published else datetime.now().date().isoformat())
                        key = f"{market}_{sector}_{file_date}.txt"
                        else:
                            sectors = ['general']
                else:
                    # åŸºç¡€åˆ†ç±»
                    market = 'INTL'
                    sectors = item.categories if item.categories else ['general']
                
                # ä¸ºæ¯ä¸ªè¯†åˆ«çš„è¡Œä¸šåˆ›å»ºæ¡ç›®
                for sector in sectors:
                    key = f"{market}_{sector}_{today}.txt"
                    if key not in classified_items:
                        classified_items[key] = []
                    
                    classified_items[key].append({
                        'title': item.title,
                        'url': item.url,
                        'summary': item.summary,
                        'content': item.full_content or item.content,
                        'published': item.published.isoformat() if item.published else today,
                    # æ–‡ä»¶å¤´æ˜¾ç¤ºæ–‡ä»¶æ—¥æœŸï¼ˆä»æ–‡ä»¶åè§£ææˆ–ç¬¬ä¸€æ¡çš„publishedï¼‰
                    header_date = items[0].get('published', '') if items else ''
                    f.write(f"International News - {header_date}\\n")
                        'author': item.author,
                        'tags': item.tags
                    })
        
        # å†™å…¥æ–‡ä»¶
        for file_name, items in classified_items.items():
            file_path = output_path / file_name
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(f"International News - {today}\\n")
                f.write("=" * 50 + "\\n\\n")
                
                for item in items:
                    f.write(f"Title: {item['title']}\\n")
                    f.write(f"URL: {item['url']}\\n")
                    f.write(f"Source: {item['source']}\\n")
                    if item.get('author'):
                        f.write(f"Author: {item['author']}\n")
                    f.write(f"Published: {item['published']}\n")
                    if item.get('tags'):
                        f.write(f"Tags: {', '.join(item['tags'])}\n")
                    
                    f.write(f"\nSummary:\n{item.get('summary', 'N/A')}\n\n")
                    
                    content = item.get('content', '')
                    if len(content) > 50:
                        f.write(f"Content:\n{content}\n")
                    else:
                        f.write(f"Content: {content or 'Content not available'}\n")
                    
                    f.write("-" * 50 + "\n\n")
            
            saved_files.append(str(file_path))
            print(f"Saved {len(items)} items to {file_path}")
        
        return saved_files

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ International News Aggregator")
    print("=" * 40)
    
    # æ£€æŸ¥ä¾èµ–
    missing_deps = []
    if not FEEDPARSER_AVAILABLE:
        missing_deps.append("feedparser")
    if not BS4_AVAILABLE:
        missing_deps.append("beautifulsoup4")
    
    if missing_deps:
        print(f"\\nâš ï¸ Missing dependencies: {', '.join(missing_deps)}")
        print("Install with: pip install feedparser beautifulsoup4")
        return
    
    aggregator = InternationalNewsAggregator()
    
    # è·å–æ‰€æœ‰æºçš„æ•°æ®
    all_items = aggregator.fetch_all_sources()
    
    if not all_items:
        print("\\nâŒ No items retrieved from any source")
        return
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    print("\\nğŸ’¾ Saving to files...")
    saved_files = aggregator.save_to_files(all_items)
    
    print(f"\\nâœ… Completed! Saved {len(saved_files)} files:")
    for file_path in saved_files:
        print(f"   ğŸ“„ {file_path}")

if __name__ == "__main__":
    main()